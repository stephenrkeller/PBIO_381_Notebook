# Ethan -- great job on this notebook. I'm really impressed with how well you documented your work, and stayed organized with your code and annotations. Nively done!  --Steve



## Author: Ethan Thibault     
## Ecological Genomics:   

### Overall Description of notebook      

This notebook will catalogue my entries throughout the semester, including all of my code, results and interpretations.

### Date started: 2018-01-04
### Date end:   (year-month-day)    

### Philosophy   
Science should be reproducible and one of the best ways to achieve this is by logging research activities in a notebook. Because science/biology has increasingly become computational, it is easier to document computational projects in an electronic form, which can be shared online through Github.    

### Helpful features of the notebook     

**It is absolutely critical for your future self and others to follow your work.**     

* The notebook is set up with a series of internal links from the table of contents.    
* All notebooks should have a table of contents which has the "Page", date, and title (information that allows the reader to understand your work).     
* Also, one of the perks of keeping all activities in a single document is that you can **search and find elements quickly**.     
* Lastly, you can share specific entries because of the three "#" automatically creates a link when the notebook renders on github.      


<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.  


### Table of contents for 60 entries (Format is *Page: Date(with year-month-day). Title*)        
* [Page 1: 2018-01-24](#id-section1). Intro to Github, RMarkdown, and UNIX command-line
* [Page 2: 2018-01-26](#id-section2). Familiarizing myself with UNIX command-line homework
* [Page 3: 2018-01-29](#id-section3). Working with RNA-Seq data (day one)
* [Page 4: 2018-01-31](#id-section4). Working with RNA-Seq data (day two)
* [Page 5: 2018-02-05](#id-section5). Working with RNA-Seq data (day three)
* [Page 6: 2018-02-06](#id-section6). Working with RNA-Seq data (day four)
* [Page 7: 2018-02-12](#id-section7). Working with RNA-Seq data (day five)
* [Page 8:](#id-section8). Working with RNA-Seq data (day six)
* [Page 9:](#id-section9). Population Genomics 1: Intro to SNP and Genotype Calling
* [Page 10:](#id-section10). Population Genomics 2: Diversity and Site Frequency Spectrum (SFS)
* [Page 11:](#id-section11). Population Genomics 3: Admixture and Population Structure
* [Page 12:](#id-section12). 
* [Page 13:](#id-section13). Code for Assignment One
* [Page 14: 2018-03-19](#id-section14). PopGenomics tutorial - Day 5
* [Page 15:2018-03-21](#id-section15). Testing selection using Fst outlier analysis
* [Page 16: 2018-03-26](#id-section16). Population Genomics 7: Analyzing Fst outlier (Bayescan) results
* [Page 17:](#id-section17). Population Genomics 7: Analyzing Fst outlier (Bayescan) results
* [Page 18:](#id-section18). Day one of project
* [Page 19:](#id-section19). bwa aln and bam to sam hw
* [Page 20:](#id-section20). Acquiring the necessary files
* [Page 21:](#id-section21). 4/23/18 Work Day
* [Page 22:](#id-section22). Project Work Day 4/25/18
* [Page 23:](#id-section23). Project 4.27.2018
* [Page 24:](#id-section24). The rest of my project through 05.08.2018
* [Page 25:](#id-section25).
* [Page 26:](#id-section26).
* [Page 27:](#id-section27).
* [Page 28:](#id-section28).
* [Page 29:](#id-section29).
* [Page 30:](#id-section30).
* [Page 31:](#id-section31).
* [Page 32:](#id-section32).
* [Page 33:](#id-section33).
* [Page 34:](#id-section34).
* [Page 35:](#id-section35).
* [Page 36:](#id-section36).
* [Page 37:](#id-section37).
* [Page 38:](#id-section38).
* [Page 39:](#id-section39).
* [Page 40:](#id-section40).
* [Page 41:](#id-section41).
* [Page 42:](#id-section42).
* [Page 43:](#id-section43).
* [Page 44:](#id-section44).
* [Page 45:](#id-section45).
* [Page 46:](#id-section46).
* [Page 47:](#id-section47).
* [Page 48:](#id-section48).
* [Page 49:](#id-section49).
* [Page 50:](#id-section50).
* [Page 51:](#id-section51).
* [Page 52:](#id-section52).
* [Page 53:](#id-section53).
* [Page 54:](#id-section54).
* [Page 55:](#id-section55).
* [Page 56:](#id-section56).
* [Page 57:](#id-section57).
* [Page 58:](#id-section58).
* [Page 59:](#id-section59).
* [Page 60:](#id-section60).

------
<div id='id-section1'/>
### Page 1: 2018-01-24. Notes on using Github, Rmarkdown, and the UNIX command-line

Today we created our GitHub repos for the course and began our notebooks.

Other goals for today:

* publish our notebooks to GitHub
* log into the UNIX server

------
<div id='id-section2'/>
### Page 2: 2018-01-26. Familiarizing myself with UNIX command-line homework

As a homework assignment, I will log back into the pbio381 server, familiarize myself with some of the commands, and make this notebook post that I will push to my GitHub notebook.

What did I do:

* open terminal
* enter "ssh eathibau@pbio381.uvm.edu" and then enter password
* next I perused my files using cd to change directory and ll to view files within those directories
* entered pwd to see current directory
* I also realized that if you want to change to a new directory, you need to be above that in the hierarchy of files. If you aren't you can use "cd .." to move back or "cd /" but this brings you really far back.
* I got lost going back to the root because my permission was denied once I started entering the letter directories so I couldn't see where to go next. Luckily in class I wrote what my ~ stood for "/users/e/a/eathibau" although now I am realizing I probably could have wrote "cd ~"
* Yes "cd ~" works from the root

At the risk of creating files that will clutter up my directories and confuse me in the future, I will abstain from doing that in this assignment and focus on moving around the directory, seeing what is there and problem solving how to move where I want to be.

------
<div id='id-section3'/>
### Page 3: Working with RNA-Seq data (day one)

#### Experimental Setup

#### Populations

* Italy (IT)
* Western Australia (WA)
* North Carolina (NC)

#### Developmental Stages

* L3L
* PP1
* PD1
* AD4

#### Sex

* Male
* Female

3 pops X 4 developmental stages X 2 sexes X 3 individuals = 72 samples

Sequenced on about 7 lanes of Illumina HiSeq 2500

#### Viewing File

View first four lines of my file:

zcat WA_PP1_F1_AGTCAA_L003_R1_001.fastq.gz | head -n4

Output:

@CCRI0219:155:C2LNBACXX:3:1101:1499:1976 1:N:0:AGTCAA
CGGGATCGTAAGGAGCTAATTCTTTAGCACGGGATGTTTTTACTAAATCAACCCATTCCGGTACTTTTAGTTTTCCTGATTTTTTAAGAAATTGGGCGAA
+
@@?DDDDDFFH??FHBHGI4CB?FHGH>@<EGG@DEHHIGIEFHE>DH9CC3B;;7@CCEBHEDEDEDE@ACAC;>@CCCCCCFBBA>@C9A:>@?B><@

Line 1: begins with @ and then information about the read

Line 2: the actual DNA sequence

Line 3: always begins with a +

Line 4: quality scores

P is the probability that a base pair call was an error and then converted to Q score on a scale of 0 to 40

#### Visualize using FastQC

This makes the fastqc file, using the path to the file and then use -o to specify an output directory (the directory must be created beforehand (using mkdir).

This gives you a summary of all the data.

fastqc /data/project_data/beetles/rawdata/WA_PP1_F1_AGTCAA_L003_R1_001.fastq.gz -o ~/Fastqc_output/

Open/view the html output file via Fetch

* pbio381.uvm.edu
* uvm netid
* password

Open/view the html output file via scp command

* Open new terminal
* change directory to where I want to download the file to my local computer
* use scp command:

ip0af527d4:~ ethanthibault$ cd /Users/ethanthibault/Documents/UVM_2018/PBIO381/PBIO_381_Notebook 
ip0af527d4:PBIO_381_Notebook ethanthibault$ scp eathibau@pbio381.uvm.edu:~/Fastqc_output/WA_PP1_F1_AGTCAA_L003_R1_001_fastqc.html .
eathibau@pbio381.uvm.edu's password: 
WA_PP1_F1_AGTCAA_L003_R1_001_fastqc.html      100%  350KB   6.8MB/s   00:00    
ip0af527d4:PBIO_381_Notebook ethanthibault$ 

* the (.) after the html file places it in my current directory where I can now open it

------
<div id='id-section4'/>
### Page 4: Working with RNA-Seq data (day two)

Use "vim" command to open up and view trim_example.sh file

cp /data/scripts/trim_example.sh ~/scripts/

Edit things in vim using "i" then hit esc (leaves edit mode) and enter ":wq" to write and escape.

* ":" allows you to enter commands
* "w" allows you to write
* "q" quits vim
* "w trim_F1.sh" allows you to rename file

The new file was written to the current directory and use the following code to make it executable:

chmod u+x trim_F1.sh

* "u" means owner
* "x" makes it executable

Making scripts like this allows you to run one command in the terminal to execute many different lines of code.

Use fastqc command to view these files.

Use Fetch to open it, but also scp to move it to my local drive:

scp eathibau@pbio381.uvm.edu:~/Fastqc_output/*html .

#### Mapping Reads

* Download reference transcriptome and make an index (Steve and Melissa already did this)
* Enter Screen with "screen"
* Run your command
* leave screen with "cntl+a+d"

This allows you to quit terminal and close/shutoff computer without it ending

------
<div id='id-section5'/>
### Page 5: Working with RNA-Seq data (day three)

- Save last 100 lines as tail.sam
- open with vim
- get rid of wrapping in vim

tail -n 100 WA_PP1_F1_bwamem.sam > tail.sam

vim tail.sam

:set nowrap

#### This is how you interpret the sam file

In order of columns from left to right

- the read, aka. query, name,
- a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
- the reference sequence name to which the read mapped
- the leftmost position in the reference where the read mapped
- the mapping quality (Phred-scaled)
- a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
- an ‘=’, mate position, inferred insert size (columns 7,8,9),
- the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
- then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).

Use grep to try and find flags

grep -c X0:i:1 WA_PP1_F1_bwamem.sam

The -c gives you the count of that flag

Use "man" to get the manual for a command

man grep





Use samtools to see a summary of how well our reads mapped to the reference

samtools flagstat WA_PP1_F1_bwamem.sam

------
<div id='id-section6'/>
### Page 6: Working with RNA-Seq data (day four)

BASH

- make a bash file "vim test_bash"
- shift i allows you to alter the script
- put "#! /bin/bash/"
- type stuff
- alter accessibility "chmod u+x test_bash"
- open file "./test_bash"  the . is the current directory
- rename it with the .sh at the end "mv ./test_bash ./test_bash.sh"

To move the two files from the server to my local computer I open a new terminal, drag the folder to set directory.

------
<div id='id-section7'/>
### Page 7: Working with RNA-Seq (day five)

Here is my current code for the the DESeq2. I did not have what Melissa had on the projector.

If that is too difficult to read, [this](DESeq2.R) is hopefully a functional link to my R code



##### set working directory

setwd("~/Documents/UVM_2018/PBIO381/PBIO_381_Notebook")

library("DESeq2")

library("ggplot2")

countsTable <- read.delim("allcountsdataRN_noIT.txt", header=TRUE, stringsAsFactors = TRUE, row.names = 1)

countData <- as.matrix(countsTable)

head(countData)

conds <- read.delim("cols_data_noIT.txt", header=TRUE, stringsAsFactors =  TRUE, row.names = 1)

head(conds)

colData <- as.data.frame(conds)

#########################################

dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~devstage + sex + population)

##### "population effect" model controlling for differences in devstage and sex

dim(dds)

##### [1] 17483    48

dds <- dds[rowSums(counts(dds)) > 1,]

dim(dds)

##### [1] 16851    48     Okay, nice, only lost about 600 genes

dds <- DESeq(dds, modelMatrixType = "standard")

resultsNames(dds)

##### [1] "Intercept"           "devstage_L3L_vs_AD4" "devstage_PD1_vs_AD4" "devstage_PP1_vs_AD4" "sex_M_vs_F"    

##### [6] "population_WA_vs_NC"

res <- results(dds)

str(res)

res <- res[order(res$padj),]

head(res)

##### log2 fold change (MLE): population WA vs NC 
##### Wald test p-value: population WA vs NC 
#####DataFrame with 6 rows and 6 columns
##### baseMean log2FoldChange     lfcSE      stat       pvalue         padj
##### <numeric>      <numeric> <numeric> <numeric>    <numeric>    <numeric>
#####   OTAU017482-RA 126.291608     -5.4166340 0.6732174 -8.045892 8.561964e-16 1.268369e-11
##### OTAU012716-RA 188.877675      4.2644034 0.5427535  7.856980 3.935069e-15 2.914706e-11
##### OTAU008667-RA 231.871115     -0.8736955 0.1267134 -6.895051 5.384538e-12 1.994164e-08
##### OTAU012562-RA 251.774364     -0.8774079 0.1270957 -6.903520 5.072966e-12 1.994164e-08
##### OTAU013988-RA   4.416955      4.4229857 0.6836393  6.469765 9.815559e-11 2.908154e-07
##### OTAU011160-RA  10.241516     -2.5149390 0.4125305 -6.096371 1.085033e-09 2.678947e-06

summary(res)

##### out of 16851 with nonzero total read count
##### adjusted p-value < 0.1
##### LFC > 0 (up)     : 333, 2% 
##### LFC < 0 (down)   : 282, 1.7% 
##### outliers [1]     : 85, 0.5% 
##### low counts [2]   : 1952, 12% 
##### (mean count < 1)
##### [1] see 'cooksCutoff' argument of ?results
##### [2] see 'independentFiltering' argument of ?results

res_pop <- results(dds, name="population_WA_vs_NC", alpha=0.05)

res_pop <- res_pop[order(res_pop$padj),]

summary(res_pop)

######################## Data visualization

plotMA(res_pop, main="DESeq2",ylim=c(-2,2))

abline(h=c(-1,1), col="blue", lwd=2)

##### sex effect?

res_sex <- results(dds, name="sex_M_vs_F", alpha=0.5)

plotMA(res_sex, main="DESeq2",ylim=c(-2,2))

abline(h=c(-1,1), col="blue", lwd=2)

######## PCA

vsd <- vst(dds,blind=FALSE)

data <- plotPCA(vsd,intgroup=c("population","devstage","sex"), returnData=TRUE)

percentVar <- round(100 * attr(data, "percentVar"))

data$devstage <- factor(data$devstage, levels=c("L3L","PP1","PD1","AD4"), labels = c("L3L","PP1","PD1","AD4"))

pdf(file="PCA_sex_by_stage.pdf",height=5.5,width=5.5)

ggplot(data,aes(PC1,PC2,color=sex,shape=devstage)) +
  geom_point(size=4,alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()
  
dev.off()

ggplot(data,aes(PC1,PC2,color=population,shape=devstage)) +
  geom_point(size=4,alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()**



------
<div id='id-section8'/>
### Page 8: Working with RNA-Seq data (day sex)



------
<div id='id-section9'/>
### Page 9: Population Genomics 1: Intro to SNP and Genotype Calling

Goals:

1. Take sequence alignment files (sam) and extract mapping stats & read depth coverage
2. Convert sam to binary (bam) format, and use samtools/bcftools for SNP detection
3. Learn Variant Call Format (vcf) for storing SNP genotypes
4. Begin to evaluate filtering strategies for determining high-quality SNPs for downstream analyses

Where we've been:

Started with raw reads. Then using trimmomatic produced clean reads. Next we mapped to the reference genome using bwa to get sam files. Then we used the python script to obtain a counts file. Next, we used DESeq2 to identify differentially expressed genes. Then we did companion analyses like enrichment and WGCNA.

Now we are going back to our sam files for pop gen studies. First we will call SNPs and genotypes. We can use samtools (used more frequently) and reads2SNPs (used less frequently but has handy paralog filtering). Then once we have SNPs we need to use quality control to identify useful SNPs and genotypes with vcftools. Once we have SNPs and genotypes that are reliable we can find pop gen statistics like pi, Tajima's D, structure selection etc.

1. Convert from sam >> bam (binary file that is more condensed but unreadable)
2. Check mapping stats
3. Fix reads that are no longer mated (paired)
4. Remove duplicates (takes awhile)
5. Index for computational efficiency
6. Use the companion program bcftools to call SNPs and genotypes

Move to folder with sam file

"WA_PP1_F1_bwamem.sam"

type vi to open vim and type i to edit the form.

put in shebang "#!/bin/bash" This tells it to read it in bash language

samtools (use samtools program) view (opens file) -b (export in binary) -@ 4 (use four CPUs) WA_PP1_F1_bwamem.sam (file name) -o (output file name) WA_PP1_F1_bwamem.bam

Steve provided a manual for samtools on the github page.

samtools flagstat (gives you a summary of the kinds of fixes that need to be done) WA_PP1_F1_bwamem.bam (filename)

samtools depth WA_PP1_F1_bwamem.bam (gives you the number of reads at each position)

samtools fixmate WA_PP1_F1_bwamem.bam WA_PP1_F1_bwamem.fixmate.bam (this fixes mate pais between paired end reads that have been orphaned, input file then output file)

samtools sort -@4 WA_PP1_F1_bwamem.fixmate.bam WA_PP1_F1_bwamem.fixmate.sorted.bam (sort alignments using 4 CPUs: input file then output file)

samtools rmdup WA_PP1_F1_bwamem.fixmate.sorted.bam WA_PP1_F1_bwamem.fixmate.sorted.rmdup.bam (this removes pcr duplicates with input and output file. we aren't doing it becasue it takes a while but should typically do it)

samtools index WA_PP1_F1_bwamem.fixmate.sorted.bam (this indexes our final alignment for fast processing)

hit escape, ":w CallSNPs.sh" to write this bash file

":q" to quit

chmod o+x CallSNPs.sh (as owner add executable function to our file)

Enter a screen to run this file in the background *control AD to quit screen

"screen"

"bash CallSNPs.sh" (run the script)

use *control AD to exit screen

samtools tview WA_PP1_F1_bwamem.fixmate.sorted.bam /data/project_data/beetles/reference/OTAU.fna (maps the reads to the reference genome)

------
<div id='id-section10'/>
### Page 10: Population Genomics 2: Diversity and Site Frequency Spectrum (SFS)

Gives you a breakdown of all of you reads:

samtools flagstat WA_PP1_F1_bwamem.fixmate.sorted.bam

Now we are going to call SNPS in our bash file

vi CallSNPs.sh

bcftools mpileup \
(gives you how many snps at each position and removes invariant sites)

A backslash (top left goes back) allows you to move to the next line

-Ou -f /data/project_data/beetles/reference/OTAU.fna \
(output data in uncompressed form)

(where to get sorted bam file)

(threads 4, skip any indels ask for allele depth and total depth)

"|" is a pipe that takes everything before it and sends it to a new command.

"call" does its best to determine genotype of each SNP

"Ov" file form

"mv" allows there to be more alleles than one at a certain site

"--format-fields GQ,GP" in phred scale gives you genotype quality

Phred (this is log base 10) --> -10log(probability correct)

">WA_PP1_F1.vcf" (output file)

bcftools mpileup \
        -Ou -f /data/project_data/beetles/reference/OTAU.fna \
        ~/mydata/sam/WA_PP1_F1_fixmate.sorted.bam \
        --threads 4 --skip-indel --annotate AD,DP | \
        bcftools call -Ov -mv --format-fields GQ >WA_PP1_F1.vcf

Now we want to run this:

bash CallSNPs.sh

VCF is the standard form for SNP data

This is where the SNP data lives:

/data/project_data/beetles/snps

-- is how to use options in vcftools

To view what is in the file without making any changes:

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf 
Filter out sites with more than two alleles (again just prints what you have but doesn't change the file)

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2

Filter for 90% genotype quality (10 on Phred scale) (doesn't remove the data, just codes it as missing data)

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --minGQ 10

Filter for missing data (eliminate SNPs that don't have at least 80% present data)

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --minGQ 10 --max-missing 0.8

Add filter to remove any alleles present as only one heterozygote (1/144) (minor allele frequency filter)

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --minGQ 10 --max-missing 0.8 --maf 0.01

Write a new file

vcftools --vcf /data/project_data/beetles/snps/OTAU_2018_samtools.vcf --min-alleles 2 --max-alleles 2 --minGQ 10 --max-missing 0.8 --maf 0.01 --recode --out OTAU_2018_samtoolsFILTERED.vcf



------
<div id='id-section11'/>
### Page 11: Population Genomics 3: Admixture and Population Structure

Each person is testing vcf file to test outputs of samtools vs read2snps and different max missing values. Use parameters from last time but use max missing of one and --het gives summary file of genome wide heterozygosity for each individual to see which way worked best. --out produces output file

vcftools --vcf OTAU_2018_samtools.vcf  --min-alleles 2 --max-alleles 2 --maf 0.01 --minGQ 20 --max-missing 1.0 --het --out ~/myresults/samtoolsmiss1.0 

Use R from the command line to look at summary het files

"R"

"df <- read.table("samtoolsmiss1.0.het",header=T)"

"str(df)"

The F row will tell us how heterozygous it is. more negative is totally het and randomly mating F would be roughly 0. We should probably work with read2SNPs with 0.8 max missing filter

"quit()" gets you out of R

"--recode" writes new file

This is ultimately the code we ran and saved for moving forward:

vcftools --vcf OTAU_2018_reads2snps_DP10GP95.vcf  --min-alleles 2 --max-alleles 2 --maf 0.01 --max-missing 0.8 --recode --out  ~/myresults/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf

Start SFS (site frequency spectra)

We are going to do it by population

use grep to grab all individuals:

"grep "IT" /data/project_data/beetles/metadata/cols_data.txt"

saves file and use cut to take what i grabbed with grep and just one column

"grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1"

give it a file to save as

"grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 >IT.inds"

Use vcf tools to pull sites only from IT individuals and calculate frequencies. out of IT makes sure IT goes into output file name

"vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf  --keep IT.inds --freq2 --out IT"

------
<div id='id-section12'/>
### Page 12: Population Genomics 4: Admixture and Population Structure Continued

First thing is to grep and get freqs for all three populations like at the end of last class and use scp to move them to my working directory so I can work with them in R.

ip0af52337:~ ethanthibault$ cd /Users/ethanthibault/Documents/UVM_2018/PBIO381/PBIO_381_Notebook 
ip0af52337:PBIO_381_Notebook ethanthibault$ scp eathibau@pbio381.uvm.edu:~/myresults/*frq .
eathibau@pbio381.uvm.edu's password: 
IT.frq                                        100% 5096KB   5.0MB/s   00:00    
NC.frq                                        100% 4844KB   4.9MB/s   00:00    
WA.frq                                        100% 4958KB   5.0MB/s   00:00    
ip0af52337:PBIO_381_Notebook ethanthibault$ 

In new R script we are graphing SFS. There appear to be differences between the three populations in the SFS plots that suggest different population histories. Now we should run admixture to see if there is evidence for distinct ancestries.

Go to terminal, take our vcf file and thin to remove potential linkage disequilibrium, convert from vcf to a .geno file that admixture requires. Then write a bash script to run admixture with K=1 to K=10 to see where the data tells us where population structure exists. Then take that output back into R. (This is the workflow)

Thin to get SNPs to make sure they aren't close together and would fall under linkage disequilibrium.  (--thin 1000) Within a 1000bp window pick a snp and the next snp needs to be at least 1000bp apart.

vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --thin 1000 --recode

In Java run PGDSpider in order to convert out file types into many different kinds of data forms.

Asks for beetle.pop file, beetle.spid file, and then bash script to run commands. (Steve made these files)

copy files into myscripts:

cp /data/project_data/beetles/metadata/beetles.pop .

cp /data/scripts/beetle.spid .

cp /data/scripts/vcf2geno.sh .

Change path with beetle.spid file to where we want output files to go (change mydata to myscripts)

open bash script and the first thing you can change is how many Gigabytes of space you give the program to run. (-Xmx2G)

make bash script to cycle through all 10 K. j gives it 4 threads of CPU to work. cv is cross values. putting the CV values in the txt file allows you to pick the smallest value (least error).

"#!/bin/bash

for K in {1..10}

do

admixture -j 4 --cv 10 ~/myresults/out.recode.vcf.geno $K | tee log${K}.out

done

grep "CV" log*out > chooseK.txt"

Change the permissions:

chmod u+x ADMIX.sh



------
<div id='id-section13'/>
### Page 13: Code for Assignment One

[Here](AssignmentOneCode.html) is a link to the code for assignment one.

------
<div id='id-section14'/>
### Page 14: PopGenomics tutorial - Day 5

First, convert vcf file to .geno file using two files necessary for the transformation.

Move the two files to my directory.

When you open the .spid file you need to put the name of the .pop file and it CANNOT be a pathway.

Open the bash script (vcf2geno.sh)

Run the bash script

bash vcf2geno.sh 

Reverse sort to see most recent files at bottom of ll

ll -rt

head out.recode.vcf.geno 

229922222222222222929222109112221219211221921119922292129229222222922222
292929222222222212222222229222229222222222222222212992229229222222922222
299111111221122222222922122902222219112922221122122192222229202129112219
112111212111212121011221221222211222221221212221191990129111222211110122
222222122222222211222222122222222222222222212222222222222222222222222222
222212212221212222222120111022222200011022200220092192222221122222222122
221111112211211221021110000001111200000010100100122122111120121211110010
222922222222222212222992219222222222222922222121222992222222222222222221
222222212212222222992992112222211229222222229919022122229922212129922229
922222212122221229192292222292222229121122212222291292299219011121122012

Genotypes --> Ref:A, Alt:T

0 = AA

1 = AT

2 = TT

9 = "" Missing Data

Open Admixture bash script to continue editing:

Use a loop to run an admixture for every value of k 1 through 10. We need to remove the space between j and 4 and ad an = between cv and 10 and remove space between > and choose (what caused an issue last time)

for K in {1..10}

do

admixture -j4 --cv=10 ~/myresults/out.recode.vcf.geno $K | tee log${K}.out

CV flag is for cross validation.

done

grep "CV" log*out >chooseK.txt

"tee" takes everything that prints and put it into a log file

Cross validation takes all individuals and breaks it into 10 groups and masks one at a time. Trains the model on the 9 unmasked than takes the masked one and splits it into the other ten groups.

Low CV is good and means there is low mismatch of where those masked individuals fall. When K is supported, the values for CV will be low.

Search for CV in log.out files, pull out the value and put into new file. The default for grep is to return the whole line.

Use pound sign to annotate bash files

.P files are allele frequencies

.Q files are the ancestry coefficients

.out file is the logged output (CV, likelihood, Fst values)

The lowest CV values are around K2,3,4

scp the files to my folder:

Q files

col_data.txt




------
<div id='id-section15'/>
### Page 15: Testing selection using Fst outlier analysis

Type "bayescan" in terminal and see all the options for the function

-pr_odds --> we need to set a prior of the likelihood of nuetrality at a locus/chance of being locally adapted.

We will all pick different levels of prior odds to see how our results will change in what snps will be picked out of as being under selection. I am setting a value of 10,000

# Make a bash script:

open new bash script with "vi"

#!/bin/bash

# This script will pull in the bayescan input file, run bayescan, using 4 CPUs and 10000 as a prior
 odds. It will save my output file under myresults. I will also save my
bayescan /data/project_data/beetles/snps/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf.bayescan -od ~/myresults -threads 4 -pr_odds 10000

:w ~/myscripts/bayescan10000.sh

I also added a for loop to run the script three times with a thinning value of 10, 20, and 30.

#!/bin/bash

# This script will pull in the bayescan input file, run bayescan, using 4 CPUs and 10000 as a prior odds. It will save my output file under myresults.
# Use a for loop to run this multiple time with different thinning values

for i in 10 20 30

do

bayescan /data/project_data/beetles/snps/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf.bayescan -od ~/myresults -threads 4 -pr_odds 10000 -thin $i -o bayescan10000"$i"

done

go to myscripts and change the file so I can execute it.

chmod u+x bayescan.sh

Run this in a screen because it will take a few days to run.

to enter screen: screen

to exit screen ctrl a + d

to reattach: screen -r


------
<div id='id-section16'/>
### Page 16: Population Genomics 7: Analyzing Fst outlier (Bayescan) results

First we want to verify our outputs.

Run MCMC to verify our outputs.

AccRte --> acceptance rate

baye_fst --> alpha values and posterior probability

sel --> likelihood values from MCMC values

baye_Verif --> like a log file

I will be using one of Morgans runs (she had a prior odds of 10)

I moved the fst and .sel file to my directory with scp

Make a file with chromosome and position of snps

*vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --kept-sites*

## Assignment 2!

[Here is the link](Assignment2_Demographic_History.html) to the code for assignment 2.

------
<div id='id-section17'/>
### Page 17: Population Genomics 7: Analyzing Fst outlier (Bayescan) results

Notes in bayescan r script file.

To move a file from my computer to the server do in my computer terminal

scp bayescan_pr10000_candsnps.txt eathibau@pbio381.uvm.edu:~/myresults

search for the chrom ids in the models file(reference), then pipe that, search that for gene

grep -f bayescan_pr10000_candsnps.txt.unique /data/project_data/beetles/reference/OTAU.Models.gff3 | grep "gene" >

------
<div id='id-section18'/>
### Page 18: Day one of project

First we need to collect all of our RNAseq files.

Melissa supplied us with trimmed and paired fastq files for the three O. taurus populations (IT, NC, WA). We are also downloading fastq files for Oryctes borbonicus. Melissa will supply us with the fastq for O. nigriventris and O. sagittarius.

The three populations can be immediately plugged into bwaaln, but we will have to trim and clean the other files.

------
<div id='id-section19'/>
### Page 19: bwa aln and bam to sam hw

4.14.2018

For class on Monday I need to run bwa aln on the trimmed fastq files that Melissa put on the server. I also need to convert the resulting sam files to bam files.

This is what I ran:

[eathibau@pbio381 ~]$ bwa aln /data/project_data/beetles/reference/OTAU.fna /data/project_data/beetles/rawdata/NC* > ~/myproject/results/NCbwaaln.sam

I needed to pull the WA files from two places

bwa aln /data/project_data/beetles/reference/OTAU.fna /data/project_data/beetles/rawdata/WA* /data/project_data/beetles/rawdata/IT/WA* > ~/myproject/results/WAbwaaln.sam

bwa aln /data/project_data/beetles/reference/OTAU.fna /data/project_data/beetles/rawdata/IT/IT* > ~/myproject/results/ITbwaaln.sam

Finally, I need to convert my sam files to bam files:

samtools view -b -@ 4 NCbwaaln.sam -o NCbwaaln.bam


------
<div id='id-section20'/>
### Page 20: Acquiring the necessary files

Jamie needs the Bam files and I couldn't make them because you need to list all of the fastq files in order to use them (not the "*") so Melissa transfered the count data for me so I can move forward with the WGCNA but also sent us the merged bam file for Jamie.

I now have all counts data and can start moving forward with WGCNA

I discussed a lot with Melissa and it is important to think about, do I want to do WGCNA per population or one for all of the beetles. If I do it per pop it will most likely result in a venn diagram versus as a whole I can create distirbutions of centrality values with with non synonymous mutations in each population. Also something to think about, when identifying a threshold, maybe I don't want to use a threshold value because my outgroups were L3L in NC and WA but not IT because there were no L3L stage samples. By thresholding I may be biasing the data so maybe no threshold will be more appropriate...

------
<div id='id-section21'/>
### Page 21: 4/23/18 Work Day

Today I was able to fix the error I was getting when it wanted to read the data as numeric but it was coming in as an integer.

bdatExpr0[] <- lapply(bdatExpr0, as.numeric)

Spoke with steve and he thought that using the soft threshhold generated from the softthreshholding function for the pooled population would be best to move forward with (power = 12)



------
<div id='id-section22'/>
### Page 22: Project Work Day 4/25/18

I FIXED THE JAVA PROBLEM!!! I believe this is the code that did it in my terminal.

On macOS High Sierra (10.13.1) and Java Version 9 you have to use a slightly different JVM path (notice the missing jre folder in the path compared to the instructions for earlier Java versions):

sudo ln -f -s $(/usr/libexec/java_home)/lib/server/libjvm.dylib /usr/local/lib
You also have to notify R about the JVM:

ln -s /usr/local/lib/libjvm.dylib /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/
shareimprove this answer

I was also having trouble with running wgcna2igraph. To fix this problem, I needed my modules to be numbers without the MEs and I needed a color palette of 30 (using rainbow). I also needed to convert bdatExp0 into a matrix.

network <- wgcna2igraph(net = netwithpower, datExpr = as.matrix(bdatExpr0), modules2plot = modules2, colors2plot = colors)

I also figured out how to save the network object so that I could just load that each time I start working

saveRDS(network, file="networkObject.rds")

network <- readRDS(file="networkObject.rds")

------
<div id='id-section23'/>
### Page 23: Project 4.27.2018

Today Jamie and I worked on getting a list of genes with putatively deleterious snps in them based on the snpeff annotations. Most of the work done was on Jamie's computer so I am not totally sure of all of the code.

------
<div id='id-section24'/>
### Page 24: The rest of my project through 05.08.2018

After working with Jamie, the majority of my work was in an R script trying to manipulate files and tables to compare lists etc. etc. In those script files I have a running dialogue for most of the code I did in annotation form. I am still working on becoming better at annotating my code, however, this class has definitely helped me become much much better and find ways that work for me to keep up with the annotations and not get lost in the code.

Here is the first script file. This has all my work in WGCNA package, my conversion to an igraph object with the wgcna2igraph function, and the beginnings of running centrality measures.

[RScript1](MyProject.R)

The second script file is a lot of working and manipulating and visualizing the eigenvalues (but I also do this in the third script file).

[RScript2](WorkingWithEigenvalues.R)

Finally, my last R script is primarily loading other objects I have created in the other two scripts, assigning Acnestral and Derived and comparing lists upon lists upon lists upon lists... the variable names become quite exhaustive.

[RScript3](ADfinalish.R)

This is up to when we submitted the paper to you guys.

------
<div id='id-section25'/>
### Page 25:
------
<div id='id-section26'/>
### Page 26:
------
<div id='id-section27'/>
### Page 27:
------
<div id='id-section28'/>
### Page 28:
------
<div id='id-section29'/>
### Page 29:
------
<div id='id-section30'/>
### Page 30:
------
<div id='id-section31'/>
### Page 31:
------
<div id='id-section32'/>
### Page 32:
------
<div id='id-section33'/>
### Page 33:
------
<div id='id-section34'/>
### Page 34:
------
<div id='id-section35'/>
### Page 35:
------
<div id='id-section36'/>
### Page 36:
------
<div id='id-section37'/>
### Page 37:
------
<div id='id-section38'/>
### Page 38:
------
<div id='id-section39'/>
### Page 39:
------
<div id='id-section40'/>
### Page 40:
------
<div id='id-section41'/>
### Page 41:
------
<div id='id-section42'/>
### Page 42:
------
<div id='id-section43'/>
### Page 43:
------
<div id='id-section44'/>
### Page 44:
------
<div id='id-section45'/>
### Page 45:
------
<div id='id-section46'/>
### Page 46:
------
<div id='id-section47'/>
### Page 47:
------
<div id='id-section48'/>
### Page 48:
------
<div id='id-section49'/>
### Page 49:
------
<div id='id-section50'/>
### Page 50:
------
<div id='id-section51'/>
### Page 51:
------
<div id='id-section52'/>
### Page 52:
------
<div id='id-section53'/>
### Page 53:
------
<div id='id-section54'/>
### Page 54:
------
<div id='id-section55'/>
### Page 55:
------
<div id='id-section56'/>
### Page 56:
------
<div id='id-section57'/>
### Page 57:
------
<div id='id-section58'/>
### Page 58:
------
<div id='id-section59'/>
### Page 59:
------
<div id='id-section60'/>
### Page 60:

------
